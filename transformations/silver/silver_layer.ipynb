{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../config/metadata.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e36e0509-0b4a-4683-a1c0-47f96e26e5bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing Specific PySpark SQL Functions"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3979878-ede4-46e0-ac94-4f06a036f9cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setting Up Bronze and Silver Schema Configurations"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "catalog = config[\"catalog\"]\n",
    "bronze_schema = config[\"bronze_schema\"]\n",
    "silver_schema = config[\"silver_schema\"]\n",
    "email_regex = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb02776-7b64-4c4f-9442-6041800d1f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop schema\n",
    "# spark.sql(f\"DROP SCHEMA IF EXISTS {catalog}.{silver_schema} CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246e788a-3217-4273-80c1-94ed52fe5c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{silver_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014c45d5-8c97-411d-b9d2-0ee663ecc648",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning and Filtering Consumer Data"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_consumer\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_consumer\")\n",
    "string_cols = [\"name\", \"city\", \"state\", \"country\", \"gender\", \"email\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "\n",
    "df_raw = df_raw.withColumn(\"email\", regexp_extract(col(\"email\"), email_regex, 0))\n",
    "df_raw = df_raw.filter(\n",
    "    \"consumer_id IS NOT NULL AND registration_date IS NOT NULL AND age >= 18 AND email IS NOT NULL AND email != ''\"\n",
    ")\n",
    "df_raw = df_raw.drop(\"phone\", \"address\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_consumer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae795524-ea5b-4a2c-9917-42108b66c021",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transforming and Storing Consumer Invoices"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_consumer_invoice\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_consumer_invoice\")\n",
    "string_cols = [\"invoice_status\", \"payment_method\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"invoice_id IS NOT NULL AND consumer_id IS NOT NULL AND gross_amount >= 0 AND net_amount >= 0\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_consumer_invoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c72880-d1d3-4ea5-879f-2e97574cb155",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processing and Saving Cleaned Consumer Orders"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_consumer_order\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_consumer_order\")\n",
    "string_cols = [\"order_status\", \"currency\", \"payment_method\", \"channel\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"order_id IS NOT NULL AND consumer_id IS NOT NULL AND total_amount >= 0 AND order_date IS NOT NULL\")\n",
    "df_raw = df_raw.drop(\"billing_address\", \"shipping_address\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_consumer_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7666f28d-138c-4c33-b852-73a679f3525b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering and Storing Valid Consumer Order Items"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_consumer_order_items\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_consumer_order_items\")\n",
    "df_raw = df_raw.filter(\"order_item_id IS NOT NULL AND order_id IS NOT NULL AND product_id IS NOT NULL AND quantity > 0 AND unit_price > 0 AND total_price > 0\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_consumer_order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b1aac7-7cfc-4186-9c19-60de84d2a4c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning and Saving Distributor Data"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_distributor\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_distributor\")\n",
    "string_cols = [\"distributor_name\", \"company_name\", \"city\", \"state\", \"country\", \"email\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "\n",
    "df_raw = df_raw.withColumn(\"email\", regexp_extract(col(\"email\"), email_regex, 0))\n",
    "df_raw = df_raw.filter(\n",
    "    \"distributor_id IS NOT NULL AND registration_date IS NOT NULL AND email IS NOT NULL AND email != ''\"\n",
    ")\n",
    "df_raw = df_raw.drop(\"no_of_associated_deals\", \"total_open_deal_value\", \"phone\", \"address\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_distributor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ea62df-167a-4768-8c8f-bd68d3d0f38a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering and Saving Valid Distributor Invoices"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_distributor_invoice\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_distributor_invoice\")\n",
    "string_cols = [\"payment_status\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"invoice_id IS NOT NULL AND purchase_id IS NOT NULL AND amount_due >= 0 AND total_payable >= 0\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_distributor_invoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841d617d-ce5a-4df0-9651-fd990aaa59da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering and Storing Valid Purchase Items"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_distributor_purchase_items\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_distributor_purchase_items\")\n",
    "df_raw = df_raw.filter(\"purchase_item_id IS NOT NULL AND purchase_id IS NOT NULL AND product_id IS NOT NULL AND quantity_ordered > 0 AND unit_cost > 0 AND total_price > 0\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_distributor_purchase_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e95b269d-6028-4ad5-9d96-1540a3f33c56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering and Saving Cleaned Distributor Purchases"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_distributor_purchases\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_distributor_purchases\")\n",
    "string_cols = [\"order_status\", \"currency\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"purchase_id IS NOT NULL AND distributor_id IS NOT NULL AND total_amount >= 0 AND order_date IS NOT NULL\")\n",
    "df_raw = df_raw.drop(\"expected_delivery_date\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_distributor_purchases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bca560c-7709-4c82-a286-251d4ad1006e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processing and Saving Cleaned Inventory Data"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_inventory\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_inventory\")\n",
    "string_cols = [\"location_type\", \"location_name\", \"location_code\", \"city\", \"state\", \"country\", \"inventory_status\", \"last_updated\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"inventory_id IS NOT NULL AND product_id IS NOT NULL AND quantity_on_hand >= 0 AND reorder_level >= 0\")\n",
    "df_raw = df_raw.drop(\"address\", \"phone\", \"email\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_inventory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83f93c3-7c91-4141-9f48-76858e1bc616",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning and Saving Valid CPG Product Data"
    }
   },
   "outputs": [],
   "source": [
    "# cpg_product\n",
    "df_raw = spark.table(f\"{catalog}.{bronze_schema}.cpg_product\")\n",
    "string_cols = [\"product_name\", \"brand\", \"manufacturer\", \"category\", \"department\", \"description\", \"sku_id\", \"unit_of_measurement\", \"product_status\"]\n",
    "for c in string_cols:\n",
    "    df_raw = df_raw.withColumn(c, trim(col(c)))\n",
    "df_raw = df_raw.filter(\"product_id IS NOT NULL AND unit_price > 0 AND retail_price > 0\")\n",
    "df_raw = df_raw.drop(\"upc\", \"gtin\", \"expiration_days\")\n",
    "df_raw.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{silver_schema}.cpg_product\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8470931193296358,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
